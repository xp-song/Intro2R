---
title: "Basic statistics"
subtitle: "[Intro2R](https://github.com/xp-song/Intro2R) crash course: Additional material"
author: "Author: [Song, Xiao Ping](https://xp-song.github.io)"
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
    theme: paper
---

---

This worksheet provides a brief overview of basic statistics that can be computed using the R programming language.

# Data {.tabset .tabset-fade .tabset-pills}

## Import

First, load the example data `LifeCycleSavings`, which contains the savings ratio (population's personal savings divided by disposable income) aggregated for selected countries from 1960 to 1970. Five variables (columns) are reported for `r nrow(LifeCycleSavings)` countries in the world:

- `sr`: aggregate personal savings ratio (in percentage)
- `pop15`: percentage of population under 15
- `pop75`: percentage of population over 75
- `dpi`: real per-capita disposable income
- `ddpi`: percentage growth rate of `dpi`

```{r load example data}
data(LifeCycleSavings)

# convert rownames to new column
LifeCycleSavings <- cbind(country = rownames(LifeCycleSavings), LifeCycleSavings)
rownames(LifeCycleSavings) <- NULL
```

```{r quick look}
nrow(LifeCycleSavings) # number of countries
head(LifeCycleSavings) # see first few rows
```


View more details by running `?LifeCycleSavings` or `help(LifeCycleSavings)`.

---

## Distribution

Examine the frequency distribution of the personal savings ratio for all `r nrow(LifeCycleSavings)` countries:

```{r summary statistics - histogram}
sr <- LifeCycleSavings$sr # assign to variable

hist(sr)
```

Examine the minimum (`min()`) and maximum (`max()`) savings ratio across all countries.

```{r min and max}
min(sr)
max(sr)
range(sr) # calculate both min & max at once
```

Examine the savings ratio at different quantiles across the distribution:

```{r quantiles}
quantile(sr, 0.5) # 50% percentile (0.5 quantile)

quantile(sr, c(0.025, 0.25, 0.75, 0.975)) # examine multiple quantiles
```


<br>

---

# Summary statistics {.tabset .tabset-fade .tabset-pills}

## Central tendency

Explore summary statistics such as the average (`mean()`) and median (`median()`) savings ratio across all countries.

```{r summary statistics - general}
sum(sr)/length(sr) # manually calculate the mean
mean(sr) # function for arithmetic mean

median(sr) # 50% quantile, not sensitive to outliers
```

Alternatively, you can view basic summary statistics for all columns using the function `summary()`:

```{r summary stats}
summary(LifeCycleSavings) # calculate for all variables at once
```

<br>

---

## Spread

Calculate the standard deviation (σ), or dispersion around the mean. It is expressed in the same units as the original values:

```{r sd}
sqrt(sum((sr - mean(sr))^2) / (length(sr) - 1)) # manually calculate sd, by taking the 'sum of squares' divided by the 'degrees of freedom'
sd(sr) # function to calculate sd
```

Calculate the variance (σ<sup>2</sup>):

```{r variance}
sd(sr)^2 # calculate manually from the sd
var(sr)
```

Estimate the standard error:

```{r se}
sd(sr) / sqrt(length(sr)) # manually calculate
```

Note: In sampling, we measure the **standard deviation (SD)** of a sample as a representation of the spread for the entire population (which is unknown). On the other hand, the **standard error (SE)** estimates the spread across _multiple_ samples of a population, and gives us an indication of how well the sampled data represents the whole population. A higher sample size will ensure a better representation (accuracy) of the actual spread within the population.

```{r out.width = "50%", fig.align='center', dpi = 300, echo = FALSE, fig.cap="_**Figure: Difference between SD and SE.** Source: [Scribbr.com](https://www.scribbr.com/statistics/standard-error/)_"}
knitr::include_graphics("../output/sd-vs-se.png", dpi = 300)
```

The SE is often reported as confidence intervals (CIs), which tell us the range of values that an unknown population parameter (e.g., the mean) is expected to be within. It is common practice to report the CI for which 95% of all sample means are expected to be within, i.e., a 95% confidence level:

```{r ci}
sample_se <- sd(sr) / sqrt(length(sr)) # formula to calculate SE from SD


# use t-distribution to estimate population SE
# assumes that the data were randomly sampled from whole population, and that the variable follows a normal distribution!

deg_freedom <- length(sr)-1 
# 'degrees of freedom' is the max no. of values that are free to vary, in order to arrive at the estimated parameter

t_score <- qt(p = 0.975, df = deg_freedom) 
# t-score describes how far away (no. of SD) a data point is from mean of the t-distribution. We want the value for the 97.5% percentile (0.975 quantile), because a two-tailed distribution has 2.5% of data at both ends, which sums up to 5% of data being outside the CI

margin_error <- t_score * sample_se

lower_limit <- mean(sr) - margin_error
upper_limit <- mean(sr) + margin_error
c(lower_limit, upper_limit)
```

Alternatively, you can use the function `t.test()` to calculate the 95% CI:

```{r t-test}
t.test(sr, conf.level = 0.95)
```

<br>

---

# Study design {.tabset .tabset-fade .tabset-pills}

## Sampling

In experiments or observational studies, random sampling is often required to avoid bias and ensure that the sampled data accurately represents the whole population. This section demonstrates various functions that may be used to for sampling. 

First, let's 'set the seed' for R's random number generator. This will allow us to reproduce the same random sequence whenever the code is re-run in future sessions.

```{r}
set.seed(1)
```

Simple random sampling may be performed using the function `sample()`. For example, we can randomly sample 5 rows of `LifeCycleSavings` without replacement:

```{r random sampling}
rows <- nrow(LifeCycleSavings)
rows_sampled <- sample(rows, 5, replace = FALSE)
LifeCycleSavings[rows_sampled, ] # subset to sampled rows

```

Stratified random sampling can help ensure that the sampled dataset retains the same statistical information of the population. For example, we might want to choose 10 countries for further analysis, while ensuring that the savings ratio among the countries chosen retain the same distribution as the entire population:

```{r stratified sampling with distribution retained}
# categorise sr into 5 categories based on quantiles (see section 1.2)
sr_quantiles <- quantile(sr, c(0, 0.025, 0.25, 0.75, 0.975, 1))
sr_intervals <- cut(sr, breaks = sr_quantiles, include.lowest = TRUE)
LifeCycleSavings$sr_intervals <- sr_intervals # add intervals as new column
sr_frequencies <- table(sr_intervals) # count frequency of each category 
LifeCycleSavings$sr_frequencies <- sr_frequencies[LifeCycleSavings$sr_intervals] # add frequency as new column

# stratified random sampling
rows_sampled <- 
  sample(rows, 10, # choose 10 rows
         prob = LifeCycleSavings$sr_frequencies, # higher frequencies have higher probability of being chosen, retain distribution
         replace = FALSE)
LifeCycleSavings[rows_sampled, ] # subset to sampled rows

```
On the other hand, we may want _equal_ representation among categories/strata, to ensure sufficient variability in the sample. For example, we can ensure a uniform distribution in the savings ratio among the 10 countries chosen:

```{r stratified sampling with uniform distribution, message = FALSE}
library(dplyr) # load required package (install if necessary)

slice_sample(
  group_by(LifeCycleSavings, sr_intervals), # group by categories
  n = 2) # 2 countries per category (total of 10)
```

<br>

---

## Power analysis

Power analysis is typically used to understand the sample size required to detect an effect. For any of the following tests, given any three of the four factors below, we can determine the fourth:

- **Sample size**: Number of samples collected 
- **Effect size**: Magnitude of the effect of interest
- **Significance level**: Probability of finding an effect that is not there (_Type I error_)^[_Type I Error_ (alpha): Rejecting the null hypothesis when actually true (scientific consensus = 0.05)]
- **Power**: Probability of finding an effect that is there, i.e., rejecting null hypothesis when it is false (_1 - Type II error_)^[_Type II error_ (beta): Accepting the null hypothesis when it is false (scientific consensus = 0.2)]

```{r}
library(pwr) # load required package
```

There are many permutations of power calculations, depending on the factor of interest, or the type of test. Some examples to calculate the significance level are shown below:

**T-test of means between two groups:**  

```{r power for t-tests}
pwr.t.test(n = 30, # equal no. of samples between 2 groups
           d = 0.5, # effect size
           sig.level = NULL, 
           power = 0.8) 
pwr.t2n.test(n1 = 20, n2 = 30, # if sample sizes are unequal
             d =0.5, 
             sig.level = NULL, power = 0.8) 
```

**ANOVA test for means between multiple groups:**  

```{r power for anova tests}
pwr.anova.test(k = 3, # 3 groups
               n = 30, # no. of samples per grp
               f = 0.5, # effect size
               sig.level = NULL, 
               power = 0.8)
```


**Test of proportions between two groups:**  

```{r power for tests of proportions}
pwr.2p.test(n = 30, # comparing
            h = 0.5, # effect size
            sig.level = NULL, power = 0.8)
```

Refer to [statsmethods.net](https://www.statmethods.net/stats/power.html) more examples and tests (e.g., Chi-square tests, linear models, correlations).


<br>

---

# Statstical tests {.tabset .tabset-fade .tabset-pills}

Some of the following sections involve comparisons between multiple groups (factors). Let's create a new column assigning each country to a continent (group), using the package `countrycode`:

```{r add continent to data, warning = FALSE}
library(countrycode) # load required package

LifeCycleSavings$continent <- 
  countrycode(sourcevar = LifeCycleSavings$country,
              origin = "country.name",
              destination = "continent")

head(LifeCycleSavings)
```


## Correlation

Examine basic relationships between pairs of variables using the function `cor()`. Note that you can specify the type of test within the argument '`method`' (e.g. Pearson, Spearman, or Kendall correlation).

```{r basic relationships}
dpi <- LifeCycleSavings$dpi # assign disposable income to new variable
cor(sr, dpi) # correlation coefficient 'r' is weakly positive (range of -1 to 1)
cor(sr, dpi)^2 # coefficient of determination 'R squared' - percentage variation in one variable explained by other variable (range of 0-1)

cov(sr, dpi) # covariance - measure of joint variability between two numeric variables
```

The relationships between all variables (columns) can be examined using the function `plot()`:

```{r plot relationships}
plot(LifeCycleSavings[,2:5]) # only plot for main variables
```

<br>

---

## Two-group means

To compare the differences in means between two groups/samples, t-tests can be used. The following assumptions are made about the data:  

- **Independence**: Observations in one sample are independent of observations in the other sample (same data point should not appear in both samples)
- **Normality**: Both samples are approximately normally distributed
- **Homogeneity of Variances**: Both samples have approximately the same variance
- **Random Sampling**: Both samples were obtained using random sampling


```{r t-tests}
# compare sr between Europe & Asia
sr_europe <- LifeCycleSavings$sr[LifeCycleSavings$continent == "Europe"]
sr_asia <- LifeCycleSavings$sr[LifeCycleSavings$continent == "Asia"]

t.test(sr_europe, sr_asia, var.equal = TRUE) # use 'var.equal = FALSE' to perform Welch's t-test, which does assume that the 2 groups have equal variances
```

We can check if assumptions violated. For example, we can check the normality of values using the function `shapiro.test()`, or graphically using the function `qqnorm()`; points should align well with the diagonal line:

```{r check normality}
shapiro.test(sr_europe) # if p-value <0.05, data is non-normal

qqnorm(sr_europe)
qqline(sr_europe)
```

Non-parametric tests of group differences can be used if assumptions are violated. For example, these include the Mann-Whitney U, Wilcoxon Signed Rank (for paired samples), and Friedman tests (for multiple test attempts).

```{r non-parametric tests of group differences}
wilcox.test(sr_europe, sr_asia) # Mann-Whitney U test - does not assume that the two samples are normally distributed
```


<br>

---

## ANOVA/ANCOVA

We can compare the differences in means between multiple (≥ 2) groups. For example, the savings ratio between the five continents can be compared:

1. Based on an analysis of variance (ANOVA) between groups. Note that the coefficient estimate and significance level of each group is compared against the intercept, which is defined by alphabetical order (in this example, Africa)

2. Based on the analysis of covariance (ANCOVA) between groups. ANCOVA is a blend of ANOVA and linear regression, which statistically controls for the effects of other continuous variable (e.g. `dpi`) that are not of primary interest.

```{r}
model <- aov(sr ~ continent, data = LifeCycleSavings) # ANOVA
summary.lm(model) 
```



```{r}
model <- aov(sr ~ continent + dpi, data = LifeCycleSavings) # ANCOVA
summary.lm(model)
```

Importantly, ANOVA/ANCOVA also makes assumptions about the data (see Section 4.2). If assumptions are violated, for example, the (non-parametric) Kruskal–Wallis test may be used instead of ANOVA (it is an extension of the Mann–Whitney U test, which only allows for comparison between two groups). See [statsmethods.net](https://www.statmethods.net/stats/anovaAssumptions.html) for more details on how to test assumptions for ANOVA/ANCOVA.


Diagnostic plots may be used to examine whether assumptions are met. Click [here](https://arc.lib.montana.edu/book/statistics-with-r-textbook/item/57) to read more about interpreting these plots.

```{r}
par(mfrow=c(2,2)) # combine four plots into one
plot(model)
```

<br>

---

# Further resources

<br>

- [Understanding p-values through simulation](https://rpsychologist.com/pvalue/)

- [Understanding maximum likelihood](https://rpsychologist.com/likelihood/)

- [Statistical power and significance testing](https://rpsychologist.com/d3/nhst/)

- [Confidence intervals](https://rpsychologist.com/d3/ci/)

- [Correlations](https://rpsychologist.com/correlation/)
 
- [t-distribution](https://rpsychologist.com/d3/tdist/)

- [P-value distribution](https://rpsychologist.com/d3/pdist/)

- [Cohen's d](https://rpsychologist.com/cohend/)

- [Linear regression](https://mlu-explain.github.io/linear-regression/)


<br>

---

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>

Copyright © `r format(Sys.Date(), "%Y")` Song, Xiao Ping
